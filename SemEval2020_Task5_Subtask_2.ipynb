{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SemEval2020_Task5_Subtask_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlTujgsUAGFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!cp /content/gdrive/My\\ Drive/Colab\\ Notebooks/Subtask-2/train.csv /content\n",
        "!cp /content/gdrive/My\\ Drive/Colab\\ Notebooks/Subtask-2/test.csv /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB4EnsYmpn5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "prefix = '/content/'\n",
        "test_df = pd.read_csv(prefix + 'test.csv', header=None) # re-name test.csv \n",
        "test_df=test_df.drop(index=0)\n",
        "\n",
        "test_df = pd.DataFrame({\n",
        "    'id':test_df[0],\n",
        "    'text': test_df[1]\n",
        "})\n",
        "\n",
        "# display \n",
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4DctzKMLVOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install allennlp\n",
        "!pip install nltk\n",
        "!pip install tqdm\n",
        "\n",
        "!pip install -U spacy \n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IkB2ctejwTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIJgOTAePV-e",
        "colab_type": "text"
      },
      "source": [
        "START AGAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7r33OAFtOM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_modal(obj):\n",
        "    arr = list() # full set of constituents\n",
        "    modals= list() # only the modal phrases\n",
        "    pasts = list() # only the past phrases\n",
        "    nouns = list()\n",
        "    verbs = list()\n",
        "\n",
        "    def extract(obj, arr):\n",
        "        \n",
        "        if isinstance(obj, dict):\n",
        "          for k, v in obj.items():\n",
        "            if isinstance(v, (dict, list)):\n",
        "                extract(v, arr)\n",
        "              \n",
        "            if k=='nodeType' and v=='NP':\n",
        "                arr.append(v)\n",
        "            if k=='nodeType' and v=='MD':\n",
        "                arr.append(v)\n",
        "            if k=='nodeType' and v=='VP':    \n",
        "                arr.append(v)\n",
        "            if k=='nodeType' and v=='VBN': # Verb, past participle\n",
        "                arr.append(v)    \n",
        "            if k=='nodeType' and v=='VBD': # Verb, past tense\n",
        "                arr.append(v)\n",
        "            if k=='nodeType' and v=='VBP': # Verb, past tense, not 3rd person singular\n",
        "                arr.append(v)     \n",
        "            if k=='nodeType' and v=='ADVP': # Adverb Phrase\n",
        "                arr.append(v)           \n",
        "            if k=='nodeType' and v=='RB':   # Adverb \n",
        "                arr.append(v)\n",
        "            if k=='nodeType' and v=='CC':   # Coordinating conjunction\n",
        "                arr.append(v)\n",
        "            if k=='nodeType' and v=='IN':   # Preposition or subordinating conjunction \n",
        "                arr.append(v)        \n",
        "            if k=='nodeType' and v=='SBAR': # Clause starting subordinating conjunction\n",
        "                arr.append(v)\n",
        "            if k=='nodeType' and v=='SINV': # Inverted declarative sentence, i.e. one in which the subject follows the tensed verb or modal.\n",
        "                arr.append(v)\n",
        "\n",
        "        elif isinstance(obj, list):\n",
        "          prev = obj[0]\n",
        "          for item in obj:\n",
        "            if isinstance(item, dict):\n",
        "              #print(item['word'])\n",
        "              if ('would'==item['word'].lower() or 'could'==item['word'].lower() or\n",
        "                  'wouldn\\'t'==item['word'].lower() or 'could\\'t'==item['word'].lower() or\n",
        "                  'woulnd\\'t'==item['word'].lower() or 'coulnd\\'t'==item['word'].lower() or\n",
        "                  'might'==item['word'].lower() or 'should'==item['word'].lower() or\n",
        "                  'mightn\\'t'==item['word'].lower() or 'shouldn\\'t'==item['word'].lower() or\n",
        "                  'mighn\\'t'==item['word'].lower() or 'shoulnd\\'t'==item['word'].lower() or\n",
        "                  'd'==item['word'].lower() or item['word'].lower().strip().endswith('\\'d') or\n",
        "                  item['word'].lower().lstrip().startswith('as if ') or\n",
        "                  item['word'].lower().lstrip().startswith('as though ')\n",
        "              ):\n",
        "                phrase = scrap_from(obj, item)\n",
        "                modals.append({'word':item['word'],'text':phrase}) # phrase starting from item\n",
        "\n",
        "              if 'CC' in arr: \n",
        "                arr.remove('CC')   \n",
        "                if ( 'but' == prev['word'].lower() or\n",
        "                  'because' == prev['word'].lower() or\n",
        "                  'although' == prev['word'].lower()\n",
        "                ):\n",
        "                  phrase = scrap_from(obj, item)\n",
        "                  modals.append({'word':item['word'],'text':phrase}) # phrase starting from item\n",
        "\n",
        "              if 'MD' in arr: \n",
        "                arr.remove('MD')\n",
        "                for h in obj: # head word\n",
        "                  if h['word'].lower().strip()=='\\'d':\n",
        "                     modals.append({'word':h['word'],'text':obj})\n",
        "                  if h['word'].lower().lstrip().startswith('would'):\n",
        "                     modals.append({'word':h['word'],'text':obj})\n",
        "                  if h['word'].lower().lstrip().startswith('could'):\n",
        "                     modals.append({'word':h['word'],'text':obj})\n",
        "                  if h['word'].lower().lstrip().startswith('should'):\n",
        "                     modals.append({'word':h['word'],'text':obj})\n",
        "                  if h['word'].lower().lstrip().startswith('may'):\n",
        "                     modals.append({'word':h['word'],'text':obj})   \n",
        "                  if h['word'].lower().lstrip().startswith('might'):\n",
        "                     modals.append({'word':h['word'],'text':obj}) \n",
        "                  if h['word'].lower().lstrip().startswith('ought'):\n",
        "                     modals.append({'word':h['word'],'text':obj})    \n",
        "\n",
        "              if 'VBP' in arr: # common misspelling of modals\n",
        "                arr.remove('VBP')\n",
        "                if prev['word'].lower().lstrip().startswith('wouln'):\n",
        "                    modals.append({'word':prev['word'],'text':obj})\n",
        "                if prev['word'].lower().lstrip().startswith('couln'):\n",
        "                    modals.append({'word':prev['word'],'text':obj})\n",
        "                if prev['word'].lower().lstrip().startswith('shouln'): # \n",
        "                    modals.append({'word':prev['word'],'text':obj})\n",
        "                if prev['word'].lower().lstrip().startswith('mighn'):\n",
        "                    modals.append({'word':prev['word'],'text':obj}) \n",
        "                if prev['word'].lower().lstrip().startswith('oughn'):\n",
        "                    modals.append({'word':prev['word'],'text':obj})  \n",
        "\n",
        "              if 'NP' in arr: \n",
        "                arr.remove('NP')\n",
        "                nouns.append({'word':'NP','text':obj})  \n",
        "\n",
        "              if 'SINV' in arr: # Inverted declarative sentence (eg Had it been...)\n",
        "                arr.remove('SINV')\n",
        "                pasts.append({'word':'SINV','text':obj}) \n",
        "\n",
        "              if 'VP' in arr:\n",
        "                arr.remove('VP')\n",
        "                verbs.append({'word':'VP','text':obj})\n",
        "                if 'wish' == prev['word'].lower():\n",
        "                  pasts.append({'word':prev['word'],'text':obj})  \n",
        "                if 'should' in item['word'].lower():\n",
        "                  phrase = scrap_from(obj, item)\n",
        "                  pasts.append({'word':prev['word'],'text':phrase})   \n",
        "\n",
        "              if 'VBD' in arr: \n",
        "                arr.remove('VBD')\n",
        "                if 'were'==prev['word'].lower():\n",
        "                  pasts.append({'word':prev['word'],'text':obj})                  \n",
        "                if 'was'== prev['word'].lower():\n",
        "                  pasts.append({'word':prev['word'],'text':obj})  \n",
        "                if 'had'== prev['word'].lower():\n",
        "                  pasts.append({'word':prev['word'],'text':obj})\n",
        "\n",
        "              # Constituent parser mistakes 'VBN' for 'VBD'(past)\n",
        "              if 'VBN' in arr: \n",
        "                arr.remove('VBN')\n",
        "                if 'had'== prev['word'].lower():\n",
        "                  pasts.append({'word':prev['word'],'text':obj}) \n",
        "                  \n",
        "              if 'RB' in arr: \n",
        "                arr.remove('RB')\n",
        "                if 'even' == prev['word'].lower():\n",
        "                  pasts.append({'word':prev['word'],'text':obj})\n",
        "                     \n",
        "              if 'IN' in arr: \n",
        "                arr.remove('IN') \n",
        "                if ('in' == prev['word'].lower() or\n",
        "                    'if' == prev['word'].lower() or\n",
        "                    'with' == prev['word'].lower() or\n",
        "                    'without'== prev['word'].lower() or\n",
        "                    'unless'== prev['word'].lower()\n",
        "                ):    \n",
        "                  phrase = scrap_from(obj, item)\n",
        "                  pasts.append({'word':item['word'],'text':phrase})     \n",
        "                \n",
        "              #arr.append(item) # uncomment for the full set\n",
        "              #print(item)\n",
        "\n",
        "              extract(item, arr)\n",
        "              prev = item\n",
        "        return arr\n",
        "\n",
        "    results = extract(obj, arr)\n",
        "\n",
        "    return modals, pasts, nouns, verbs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MeCoOLXx8IU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scrap_from(obj, item): \n",
        "  word = str() \n",
        "  prev = str() \n",
        "  for h in obj: # head word\n",
        "    if h['word'] == item['word']: \n",
        "      word = ' ' \n",
        "    if len(word) > 0: \n",
        "      word = word.strip() \n",
        "      if (not (\n",
        "              word.lower()=='\\'m' or word.lower()=='\\'ll' or\n",
        "              word.lower()=='\\'s' or word.lower()=='d' or # I'd be...\n",
        "              word.lower()=='\\'re' or word.lower()=='\\'ve' or word.lower()=='n\\'t') and  \n",
        "          not (prev.lower().endswith('s') and word.lower()=='\\' ') and # before\\':peoples'\n",
        "          not (word.lower() in '+.: m k x' and prev[-1:].isnumeric()) and # eg:'10:30am', '3M'\n",
        "          not (prev.lower() in '+' and word.isnumeric()) and\n",
        "          not (\n",
        "              prev.lower() in '¥$€£&@({[/---#'  or  # after(eg 'S& P')\n",
        "              word.lower() in '^~*:&%)}]/---...,;' # before(eg '% 5 M')\n",
        "          )\n",
        "      ): \n",
        "        word = word + ' '\n",
        "      word = word + h['word']  \n",
        "      prev = word\n",
        "  word = normalize(word)     \n",
        "  #print(word)    \n",
        "  return word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm_UQ7ejvUsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def separate_children(obj):\n",
        "    arr = list()\n",
        "\n",
        "    def separate(obj, arr):\n",
        "        \n",
        "        if isinstance(obj, dict):\n",
        "\n",
        "            for k, v in obj.items():\n",
        "              if k != 'children':\n",
        "                if isinstance(v, (dict, list)):\n",
        "                    separate(v, arr)\n",
        "                elif k == 'word':      \n",
        "                    arr.append(v)\n",
        "\n",
        "        elif isinstance(obj, list):\n",
        "            for item in obj:\n",
        "                separate(item, arr)\n",
        "         \n",
        "        return arr\n",
        "\n",
        "    results = separate(obj, arr)\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XYFHp-5KyQ7",
        "colab": {}
      },
      "source": [
        "def extract_constituents(prediction):\n",
        " for k, v in prediction.items():\n",
        "  if (k=='hierplane_tree'):\n",
        "    for ke, val in v.items():\n",
        "      if (ke=='root'):\n",
        "        for key, value in val.items():\n",
        "          if (key == 'children'):\n",
        "            return extract_modal(value)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aVJqHY-39ny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deduplicate(past):\n",
        "  for p1 in past:\n",
        "    for p2 in past:\n",
        "      if p2 in p1 and len(p1) > len(p2):\n",
        "        if p2 in past:\n",
        "          past.remove(p2)          \n",
        "  return past         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_OCij-o85BT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_from(past, modal, last=1):\n",
        "  pst = past.copy()\n",
        "\n",
        "  for m in sorted(modal,key=len): # in order of longer consequents\n",
        "    for p in reversed(sorted(past,key=len)): # shorter antecedents\n",
        "      if p.rstrip() in m and len(m) > len(p): # p is substring of m\n",
        "        if p in past and len(past) > last: \n",
        "          past.remove(p)\n",
        "      elif m.rstrip() in p and len(p) > len(m):# m is substring of p\n",
        "        if m in modal and len(modal) > last: # can't be left empty!\n",
        "          modal.remove(m) \n",
        "\n",
        "  for p in pst: # fresh copy\n",
        "    for m in modal:\n",
        "      if m.rstrip() in p: # both p = m or p > m\n",
        "        if p in past and len(past) > last:\n",
        "          past.remove(p) # remove p{ m }\n",
        "\n",
        "  return past, modal         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFKGDv6OwVUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def separate_from(md,pa):\n",
        "  if pa in md and len(md) > len(pa):\n",
        "    if md.index(pa) > 0 :\n",
        "      md = md[0:md.index(pa)]\n",
        "      #print('|'+md+'{...}')\n",
        "    else:\n",
        "      md = md.replace(pa,'')\n",
        "      #print('|{...}'+md)  \n",
        "  return md        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pURMbvkG2iTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def simplify(constit):\n",
        "  phrase  = constit['word']\n",
        "  \n",
        "  constit = constit['text']\n",
        "  if isinstance(constit, str):\n",
        "    word = normalize(constit)\n",
        "    return word\n",
        "    \n",
        "  gist = list();\n",
        "  unique = set() \n",
        "  for c in constit:\n",
        "      p = separate_children(c) # list of c['word']s\n",
        "      for head in sorted(p,key=len):\n",
        "        #print(head) # head phrase (few words or one char ',')\n",
        "        tokenized = word_tokenize(head)\n",
        "        tokens = set(tokenized) #  without duplicates (eg ',')\n",
        "        if len(unique.intersection(tokens)) <= len(tokens):\n",
        "          unique.update(tokens) # without duplicate tokens!!\n",
        "          gist.extend(tokenized)\n",
        "  #print(gist)     \n",
        "  result = str()\n",
        "  prev = str()\n",
        "  for word in gist:\n",
        "    word = word.strip()\n",
        "    if (\n",
        "        not (\n",
        "             word.lower()=='\\'m' or word.lower()=='\\'ll' or \n",
        "             word.lower()=='\\'s' or word.lower()=='d' or # I'd be doing...\n",
        "             word.lower()=='\\'re' or word.lower()=='\\'ve' or word=='n\\'t'\n",
        "        ) and\n",
        "        not (prev.lower().endswith('s') and word.lower()=='\\' ') and # before\\':peoples'\n",
        "        not (word.lower() in '+.: m k x' and prev[-1:].isnumeric()) and # after '10:30am'\n",
        "        not (prev.lower() in '+' and word.isnumeric()) and\n",
        "        not (prev.lower()=='i' and word.lower()=='d') and # ID - identification\n",
        "        not  \n",
        "        (\n",
        "          prev.lower() in '¥$€£&@({[/---#' or  # ' ' after\n",
        "          word.lower() in '^~*:&%)}]/---...,;'# or before\n",
        "        )\n",
        "    ): \n",
        "      result = result + ' '\n",
        "      #print(prev,'-->',word)\n",
        "    prev = word\n",
        "    result = result + word \n",
        "  result = normalize(result)   \n",
        "  #print(phrase,':',result)  \n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c45HuHKFl-Ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "regex = '\\\\\\'\\s+([^\\']+)\\s+\\\\\\''\n",
        "def compact(fragment):\n",
        "# specify the number of replacements by changing the 4th argument\n",
        "  result = re.sub(regex, '\\''+r'\\1'+'\\'' , fragment)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CoPtjXKBtrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(fragment):\n",
        "  r = compact(fragment).strip()\n",
        "  r = r.replace('   ',' ') \n",
        "  r = r.replace('  ',' ')\n",
        "  r = r.replace(' ,',',') \n",
        "  r = r.replace(' .','.')\n",
        "  r = r.replace('Mr.','Mr. ')\n",
        "  r = r.replace('Ms.','Ms. ')\n",
        "  r = r.replace('Mrs.','Mrs. ')\n",
        "  r = r.replace('.  ','. ') # Mr./Ms./Mrs.\n",
        "  r = r.replace('... ','...')\n",
        "  r = r.replace(' --- ','---')\n",
        "  r = r.replace(' -- ','--')\n",
        "  r = r.replace(' - ','-')\n",
        "  r = r.replace(' !','!')\n",
        "  r = r.replace(' ?','?')\n",
        "  r = r.replace('$ ','$')\n",
        "  r = r.replace('# ','#')\n",
        "  r = r.replace(' %','%')\n",
        "  r = r.replace('( ','(')\n",
        "  r = r.replace(' )',')')\n",
        "  r = r.replace(' ;',';')\n",
        "  r = r.replace(' :',':') \n",
        "  r = r.replace(': ',':') #50/50 - carefull!\n",
        "  r = r.replace(' nt','nt')\n",
        "  r = r.replace(' (k)','(k)') #401(k)\n",
        "  r = r.replace(' (tm)','(tm)')\n",
        "  r = r.replace('n na ','nna ')\n",
        "  r = r.replace(' \\'m ','\\'m ')\n",
        "  r = r.replace('s \\' ','s\\' ') # was 'neurtralized'?\n",
        "  r = r.replace(' \\'s ','\\'s ') # a 'surgical' strike!\n",
        "  r = r.replace(' \\'d ','\\'d ')\n",
        "  r = r.replace(' \\'re ','\\'re ')\n",
        "  r = r.replace(' \\'ve ','\\'ve ')\n",
        "  r = r.replace(' \\'ll ','\\'ll ')\n",
        "  r = r.replace(' n\\'t','n\\'t') # what about UPPER()?\n",
        "  r = r.replace(' \\' ',' \\'')\n",
        "  r = r.rstrip()\n",
        "  return r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUKIAsKb9Cwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def defragment(fragments, text, original, removed):\n",
        "  result = []\n",
        "  candid = []\n",
        "  defrag = []\n",
        "  prev = str() # previous fragment\n",
        "  start = 0 # start of next fragment\n",
        "  end = -1 # end of previous fragment  \n",
        "  for f in sorted(fragments,key=len):\n",
        "    r = normalize(f)\n",
        "    #print(prev+' | '+r)\n",
        "    try: \n",
        "      start = text.index(r)\n",
        "    except ValueError:\n",
        "      #print('\\n{'+r+'} not in:\\n '+text) # this is now in offset()\n",
        "      pass\n",
        "    if start > end: # assume no overlap with prev (ie separte fragment)\n",
        "      defrag = prev + r # try to join two fragments\n",
        "      index = text.find(defrag) # is it exactly consequitive fragment?\n",
        "      if index < 0 or index > end: # it separate and non-consequitive \n",
        "        end = start + len(r) - 1\n",
        "        prev = r\n",
        "        if end < len(text):\n",
        "          candid.append(overlay(start, {'labels':0, 'text':r, 'start':start, 'end':end}, original, removed))\n",
        "      elif index > 0 and index+len(prev)+len(r) < len(text): # consequitive\n",
        "        start = index \n",
        "        end = start+len(prev)+len(r)-1\n",
        "        prev = text[start:end]\n",
        "        candid.append(overlay(start, {'labels':0, 'text':defrag, 'start':start, 'end':end}, original, removed))\n",
        "    elif start < end: # next fragment is located before prev end\n",
        "      defrag = r + prev\n",
        "      index = text.find(defrag) # is it exactly preceding prev fragment?\n",
        "      #print(text,index,defrag)\n",
        "      if index > 0 and index+len(prev)+len(r)-1 == end:\n",
        "        start = index\n",
        "        prev = text[start:end]\n",
        "        candid.append(overlay(start, {'labels':0, 'text':defrag, 'start':start, 'end':end}, original, removed))\n",
        "      elif start >= 0:\n",
        "        end = start + len(r) - 1\n",
        "        prev = r\n",
        "        if end < len(text):\n",
        "          candid.append(overlay(start, {'labels':0, 'text':r, 'start':start, 'end':end}, original, removed))\n",
        "        \n",
        "  if len(candid) == 1:\n",
        "    candid[0].update(labels = 1) # gold  \n",
        "\n",
        "  result.extend(candid)\n",
        "\n",
        "  return result       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDbpK2N_Md_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def overlay(starting, fragment, original, removed):\n",
        "  assert isinstance(fragment, dict)\n",
        "\n",
        "  base = offset(fragment['text'], starting, original)\n",
        "  text = base[0]\n",
        "  start= base[1]\n",
        "  end = base[2]\n",
        "  #print(start,text,end)\n",
        "\n",
        "  for r in removed:\n",
        "    #print(r[0],start,r[1],end)\n",
        "    if r[0] <= start:\n",
        "      start=start + 1\n",
        "      end = end + 1\n",
        "      #print(start,text,end)\n",
        "    elif r[0] <= end:\n",
        "      end = end + 1\n",
        "      #print(start,text,end) \n",
        "    \n",
        "  fragment.update(text = text)  \n",
        "  fragment.update(start=start) \n",
        "  fragment.update(end = end) \n",
        "\n",
        "  return fragment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt4ghi_XxGa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def offset(text, starting, original):\n",
        "  start = -1\n",
        "  end = -1\n",
        "  prev = str()\n",
        "  for c in text:\n",
        "    try: \n",
        "      #print(c, start, prev, end)\n",
        "      start = original.index(prev + c, starting)\n",
        "      end = start + len(prev) + len(c) - 1\n",
        "    except ValueError:\n",
        "      if c==' ': # eg.'Pfizer (NYSE: '+'PFE)'\n",
        "        last = original.find(prev, starting)\n",
        "        if last > -1: # back 1 space helped\n",
        "          prev = prev # + c\n",
        "          continue    # skip 1 space\n",
        "        else:   \n",
        "          if end >= 0: # reverse from end\n",
        "            end = end + 1 # 1 space\n",
        "            start = end - len(prev)\n",
        "            text = original[start:end]\n",
        "            #print(start, prev, end, text)\n",
        "          else:\n",
        "            print('\\n\\\"'+prev+'\\\" not from '+str(starting) + ' in:'+original)\n",
        "          return text, start, end  \n",
        "      else: \n",
        "        last = original.find(prev + ' ' + c, starting)\n",
        "        if last > -1: # add 1 space \n",
        "          prev = prev + ' ' + c\n",
        "          continue\n",
        "        else:  \n",
        "          last = original.find(prev+c)\n",
        "          if last == -1: # eg 'bla-' instead of 'bla -'\n",
        "            last = original.find(prev) # leave 'bla'\n",
        "          if last > -1: # without space!\n",
        "            start = last\n",
        "            end = start + len (prev) + len(c) \n",
        "            prev = prev + c\n",
        "            continue\n",
        "          if start >= 0: # reverse from start\n",
        "            end = start + len(prev) + len(c)\n",
        "            text = original[start:end]\n",
        "            #print(c,start,prev,end,text)\n",
        "          else:\n",
        "            print('\\n\\\"'+prev+c+'\\\" not from '+str(starting) + ' in:'+original)  \n",
        "          return text,start,end \n",
        "\n",
        "    prev = prev + c\n",
        "\n",
        "  #print(start, prev, end, text)  \n",
        "  return text, start, end  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msFC5TstNCsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def record(original):\n",
        "  base=[(i, c) for i, c in enumerate(original)]\n",
        "  return base"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-pu1f8IN7VA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def contract(text, base):\n",
        "  removed = []\n",
        "  i=0\n",
        "  for b in base: \n",
        "    if text.find(b[1],b[0]-i)==-1:\n",
        "      removed.append(b)\n",
        "      i = i + 1\n",
        "  return removed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nietMZM3C-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def chunk(original):\n",
        "  verbs = list()\n",
        "  modal = list()\n",
        "  nounm = list()\n",
        "  past = list() \n",
        "  nounp = list()\n",
        "  nouns = list()\n",
        "\n",
        "  base = record(original)\n",
        "  original = original.replace('\\\"','') # hack the \"\"\n",
        "  removed = contract(original, base)\n",
        "  \n",
        "  text = normalize(original) # remove extra spaces\n",
        "\n",
        "  prediction = predictor.predict(sentence=text)\n",
        "  mod, pas, nou, ver = extract_constituents(prediction)\n",
        "\n",
        "  for v in ver:\n",
        "    vr = simplify(v) \n",
        "    if vr not in verbs:\n",
        "      verbs.append(vr)\n",
        "\n",
        "  for u in nou:\n",
        "    nu = simplify(u) \n",
        "    if nu not in nouns:\n",
        "      nouns.append(nu) \n",
        "\n",
        "  if len(mod)==0: # if no modal('MD') detected \n",
        "    for p in pas:\n",
        "      pa = simplify(p)\n",
        "      if pa not in past:\n",
        "          past.append(pa) \n",
        "\n",
        "  for m in reversed(sorted(mod,key=len)):\n",
        "    md = simplify(m) # get one string gist\n",
        "    if len(md)==0:\n",
        "      continue\n",
        "    #print('=======================')  \n",
        "    #print(md)\n",
        "    \n",
        "    for nm in reversed(sorted(nouns,key=len)):  \n",
        "      if nm in md:   \n",
        "        nounm.append(nm)\n",
        "        nouns.remove(nm)\n",
        "\n",
        "    for p in reversed(sorted(pas,key=len)):\n",
        "      pa = simplify(p)\n",
        "      if len(pa)==0:\n",
        "        continue\n",
        "      #print('-----------------------')\n",
        "      #print(pa)\n",
        "\n",
        "      # consider join first, but how?\n",
        "      new = separate_from(md, pa)\n",
        "      if new != md and len(new) > 0:\n",
        "        if md in modal: # remove original\n",
        "            modal.remove(md) \n",
        "        md = new \n",
        "\n",
        "      new = separate_from(pa, md)\n",
        "      if new != pa and len(new) > 0:\n",
        "        if pa in past: # remove original\n",
        "            past.remove(pa) \n",
        "        pa = new \n",
        "\n",
        "      # separate NPs originating with past tense      \n",
        "      for np in reversed(sorted(nouns,key=len)):\n",
        "        if np in pa:\n",
        "          nounp.append(np)\n",
        "          nouns.remove(np)\n",
        "        \n",
        "          #new = separate_from(pa, np)\n",
        "          #if new != pa and len(new) > 0:\n",
        "          #  if pa in past: # remove original\n",
        "          #      past.remove(pa) \n",
        "          #  pa = new \n",
        "        \n",
        "      if pa not in past and len(pa) > 0:\n",
        "        past.append(pa)         \n",
        "\n",
        "    if md not in modal and len(md) > 0:        \n",
        "      modal.append(md)\n",
        "      \n",
        "  result = remove_from(modal, past)\n",
        "  modal = result[0]\n",
        "  past = result[1]\n",
        " \n",
        "  # follwoing does not affect modal, past\n",
        "  nounm = remove_from(nounm, modal,1)[0]\n",
        "  nounm = remove_from(nounm, verbs,0)[0]\n",
        "  nounp = remove_from(nounp, past, 0)[0]\n",
        "  nounp = remove_from(nounp, verbs,0)[0]\n",
        "\n",
        "  # only after nouns are removed above\n",
        "  modal = remove_from(modal, nounm)[0]\n",
        "  past = remove_from(past, nounp)[0]\n",
        "\n",
        "  verbs = deduplicate(verbs)\n",
        "  nounm = deduplicate(nounm)\n",
        "  nounp = deduplicate(nounp)\n",
        "  modal = deduplicate(modal)\n",
        "  #past = deduplicate(past)\n",
        "\n",
        "  # get start, end positions \n",
        "  modals = defragment(modal, text, original, removed)\n",
        "  #if len(modals)==0:\n",
        "  #  modals = defragment(verbs, text, original, removed)\n",
        "\n",
        "  pasts = defragment(past, text, original, removed)\n",
        "  if len(pasts)==0:\n",
        "    if len(modals) > 1:\n",
        "      pasts.append(modals.pop(0))\n",
        "      if len(modals) == 1:\n",
        "         modals[0].update(labels=1) # gold  \n",
        "    else:  \n",
        "      pasts = defragment(nouns, text, original, removed)\n",
        "\n",
        "  return modals, pasts, verbs, nouns, nounm, nounp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quzZs7aWz7-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "modals, pasts, verbs, nouns, nounm, nounp = chunk(\n",
        "  original = \"Reading between the lines: Judge Stearns used some choice descriptions for the Cape Wind opponents, calling Cape Wind's opponent's obdurate and saying that while they typically claim the mantle of environmentalism, the opponents in this case have doffed their green garb and draped themselves in the banner of free-market economics. One part of the complaint, Stearns said, referenced documents that contradict on their face a supposed fact as plead to make an argument that is misleading and ultimately untrue. In a footnote, Stearns also wrote that were he an economist or a conservationist instead of a judge, he may have rejected Cape Wind, but that elected officials, regulators and the courts have reviewed and approved the project.\"    \n",
        ")\n",
        "\n",
        "print(nouns)\n",
        "print(verbs)\n",
        "print(nounm)\n",
        "print(nounp)\n",
        "print(pasts)\n",
        "print(modals)     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRv9sHtMZQSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from time import sleep\n",
        "\n",
        "prefix = '/content/'\n",
        "test_df = pd.read_csv(prefix + 'test.csv', header=None) # re-name test.csv \n",
        "test_df = test_df.drop(index=0)\n",
        "  \n",
        "test_df = pd.DataFrame({\n",
        "    'id':test_df[0],\n",
        "    'text': test_df[1], \n",
        "    'labels': ''\n",
        "})\n",
        "\n",
        "count = test_df['id'].count()\n",
        "print(count)\n",
        "\n",
        "i = 0\n",
        "j = 0\n",
        "modal_df= pd.DataFrame(columns=['id','labels','text','start', 'end'])\n",
        "past_df = pd.DataFrame(columns=['id','labels','text','start', 'end'])\n",
        "simple_df = pd.DataFrame(columns=['id','labels','text'])\n",
        "\n",
        "for ind in tqdm(test_df.index): \n",
        "  original = test_df['text'][ind]\n",
        "  oid = test_df['id'][ind]\n",
        "\n",
        "  modals, pasts, verbs, nouns, nounm, nounp = chunk(original)\n",
        "\n",
        "  for m in modals:\n",
        "    m_row = pd.Series(data={\n",
        "      'id': test_df['id'][ind]+'C'+ str(i),\n",
        "      'labels': m['labels'],\n",
        "      'text': m['text'] ,\n",
        "      'start': m['start'],\n",
        "      'end': m['end']\n",
        "    }, name = str(i) )\n",
        "    modal_df = modal_df.append(m_row)\n",
        "    i = i+1\n",
        " \n",
        "  for p in pasts:\n",
        "    p_row = pd.Series(data={\n",
        "      'id': test_df['id'][ind]+'A'+ str(j),\n",
        "      'labels': p['labels'],\n",
        "      'text': p['text'] ,\n",
        "      'start': p['start'],\n",
        "      'end': p['end']\n",
        "    }, name = str(j) )\n",
        "    past_df = past_df.append(p_row)   \n",
        "    j = j+1   \n",
        "\n",
        "  who = str()\n",
        "  for n in nounp:\n",
        "    who = who  +  n + ' / '\n",
        "\n",
        "  what = str()\n",
        "  for nn in nounm:\n",
        "    what = what + ' / ' + nn\n",
        "\n",
        "  i = 0\n",
        "  j = 0\n",
        "  n = 0\n",
        "  for m in modals:\n",
        "    if len(pasts) > 0:\n",
        "      for p in pasts:\n",
        "        labels = str(int(m['labels'])*int(p['labels']))\n",
        "        simple = who + p['text'] + ' | ' + m['text'] + what\n",
        "        #print(labels, simple)\n",
        "        s_row = pd.Series(data={\n",
        "          'id': test_df['id'][ind]+'A'+ str(j) + 'C'+ str(i),\n",
        "          'labels': labels,\n",
        "          'text': simple\n",
        "        }, name = str(n) )\n",
        "        simple_df = simple_df.append(s_row) \n",
        "        j = j+1 \n",
        "        n = n+1\n",
        "    else:\n",
        "      s_row = pd.Series(data={\n",
        "        'id': test_df['id'][ind]+'A'+ str(j) + 'C'+ str(i),\n",
        "        'labels':m['labels'],\n",
        "        'text': who + ' | ' + m['text'] + what\n",
        "      }, name = str(n) )\n",
        "      simple_df = simple_df.append(s_row) \n",
        "    i = i+1\n",
        "    n = n+1\n",
        "\n",
        "  sleep(1)  \n",
        "\n",
        "simple_df.head(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAQE-GxfQ4Ev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "past_df.to_csv(prefix+'antecedents.tsv', sep='\\t', index=False, header=False)\n",
        "modal_df.to_csv(prefix+'consequents.tsv', sep='\\t', index=False, header=False)\n",
        "simple_df.to_csv(prefix+'simplified.tsv', sep='\\t', index=False, header=False)\n",
        "simple_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3w26wdnuriO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv /content/antecedents.tsv /content/gdrive/My\\ Drive/Colab\\ Notebooks/Subtask-2/\n",
        "!mv /content/consequents.tsv /content/gdrive/My\\ Drive/Colab\\ Notebooks/Subtask-2/\n",
        "!mv /content/simplified.tsv /content/gdrive/My\\ Drive/Colab\\ Notebooks/Subtask-2/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SemEval2020_Task5_Subtask_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlTujgsUAGFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!cp /content/gdrive/My\\ Drive/Colab\\ Notebooks/Subtask-1/train.csv /content\n",
        "!cp /content/gdrive/My\\ Drive/Colab\\ Notebooks/Subtask-2/test.csv /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB4EnsYmpn5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "prefix = '/content/'\n",
        "test_df = pd.read_csv(prefix + 'test.csv', header=None)\n",
        "test_df=test_df.drop(index=0)\n",
        "\n",
        "test_df = pd.DataFrame({\n",
        "    'id':test_df[0],\n",
        "    'text': test_df[1]\n",
        "})\n",
        "\n",
        "# display \n",
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4DctzKMLVOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install allennlp\n",
        "!pip install nltk\n",
        "!pip install tqdm\n",
        "\n",
        "!pip install -U spacy \n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IkB2ctejwTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIJgOTAePV-e",
        "colab_type": "text"
      },
      "source": [
        "START AGAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7r33OAFtOM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_modal(obj):\n",
        "    arr = list() # full set of constituents\n",
        "    modal= list() # only the modal phrases\n",
        "    past = list() # only the past phrases\n",
        "    noun = list()\n",
        "    verb = list()\n",
        "\n",
        "    def extract(obj, arr):\n",
        "        \n",
        "        if isinstance(obj, dict):\n",
        "            for k, v in obj.items():\n",
        "                if isinstance(v, (dict, list)):\n",
        "                    extract(v, arr)\n",
        "                  \n",
        "                if k=='nodeType' and v=='NP':\n",
        "                    arr.append(v)\n",
        "                if k=='nodeType' and v=='MD':\n",
        "                    arr.append(v)\n",
        "                if k=='nodeType' and v=='VP':    \n",
        "                    arr.append(v)\n",
        "                if k=='nodeType' and v=='VBD': # Verb, past tense\n",
        "                    arr.append(v) \n",
        "                if k=='nodeType' and v=='ADVP': # Adverb Phrase\n",
        "                    arr.append(v)           \n",
        "                if k=='nodeType' and v=='RB':   # Adverb \n",
        "                    arr.append(v)\n",
        "                if k=='nodeType' and v=='CC':   # Coordinating conjunction\n",
        "                    arr.append(v)\n",
        "                if k=='nodeType' and v=='IN':   # Preposition or subordinating conjunction \n",
        "                    arr.append(v)        \n",
        "                if k=='nodeType' and v=='SBAR': # Clause starting subordinating conjunction\n",
        "                    arr.append(v) \n",
        "\n",
        "        elif isinstance(obj, list):\n",
        "            for item in obj:\n",
        "\n",
        "              if isinstance(item, dict):\n",
        "                if 'would'==item['word'].lower() or 'd'==item['word'].lower():\n",
        "                  phrase = scrap_from(obj, item)\n",
        "                  modal.append(phrase) # only phrase on the right from item\n",
        "                if 'could'==item['word'].lower():\n",
        "                  phrase = scrap_from(obj, item)\n",
        "                  modal.append(phrase) # only phrase on the right from item  \n",
        "\n",
        "                if 'MD' in arr: \n",
        "                  arr.remove('MD')\n",
        "                  for f in obj: # re-visit each fragment \n",
        "                    if f['word'].lower().lstrip().endswith(' d'):\n",
        "                      modal.append(obj)\n",
        "                    if f['word'].lower().lstrip().startswith('would'):\n",
        "                      modal.append(obj)\n",
        "                    if f['word'].lower().lstrip().startswith('could'):\n",
        "                      modal.append(obj)\n",
        "                    if f['word'].lower().lstrip().startswith('should'):\n",
        "                      modal.append(obj)\n",
        "                    if f['word'].lower().lstrip().startswith('might'):\n",
        "                      modal.append(obj)  \n",
        "\n",
        "                if 'NP' in arr: \n",
        "                  arr.remove('NP')\n",
        "                  noun.append(obj)  \n",
        "\n",
        "                if 'VP' in arr:\n",
        "                  arr.remove('VP')\n",
        "                  #verb.append(obj) # can subsume 'MD'\n",
        "                  if 'wish' == obj[0]['word'].lower():\n",
        "                    past.append(obj)  \n",
        "                  if 'should' in item['word'].lower():\n",
        "                    phrase = scrap_from(obj, item)\n",
        "                    past.append(phrase)   \n",
        "\n",
        "                if 'VBD' in arr:\n",
        "                  arr.remove('VBD')\n",
        "                  if 'were' in obj[0]['word'].lower():\n",
        "                    past.append(obj)\n",
        "                  if 'was' in obj[0]['word'].lower():\n",
        "                    past.append(obj)  \n",
        "                  if 'had' in obj[0]['word'].lower():\n",
        "                    past.append(obj)\n",
        "            \n",
        "                if 'RB' in arr: \n",
        "                  arr.remove('RB')\n",
        "                  if 'even' == obj[0]['word'].lower():\n",
        "                    past.append(obj)\n",
        "                #  if 'instead' in obj[0]['word'].lower():\n",
        "                #    past.append(obj)\n",
        "                #  if 'whichever' in obj[0]['word'].lower():\n",
        "                #    past.append(obj)  \n",
        "                #  if 'whatever' in obj[0]['word'].lower(): \n",
        "                #    past.append(obj)\n",
        "                #  if 'however' in obj[0]['word'].lower(): \n",
        "                #    past.append(obj)\n",
        "                #  if 'regardless' in obj[0]['word'].lower(): \n",
        "                #    past.append(obj)\n",
        "                #  if 'notwithstanding' in obj[0]['word'].lower(): \n",
        "                #    past.append(obj) \n",
        "\n",
        "                #if 'SBAR' in arr: # seems redundant but keep in case\n",
        "                #   arr.remove('SBAR')\n",
        "                #   for s in obj: # re-visit each sentece in the SBAR\n",
        "                #     if s['word'].lower().lstrip().startswith('if '):\n",
        "                #       past.append(s)       \n",
        "                if 'IN' in arr: \n",
        "                    arr.remove('IN') \n",
        "                    if 'in' == obj[0]['word'].lower():\n",
        "                      past.append(obj)  \n",
        "                    if 'if' == obj[0]['word'].lower(): # space after 'if'\n",
        "                      past.append(obj)\n",
        "                    if 'with' == obj[0]['word'].lower():\n",
        "                      past.append(obj) \n",
        "                    #if 'for' == obj[0]['word'].lower(): # \"for example\"\n",
        "                    #  past.append(obj) \n",
        "\n",
        "                if 'CC' in arr: \n",
        "                    arr.remove('CC')   \n",
        "                    if 'but' == obj[0]['word'].lower():\n",
        "                      past.append(obj)   \n",
        "                    if 'because' == obj[0]['word'].lower():\n",
        "                      past.append(obj)  \n",
        "                    if 'although' == obj[0]['word'].lower():\n",
        "                      past.append(obj)\n",
        "              \n",
        "              #arr.append(item) # uncomment for the full set\n",
        "              #print(item)\n",
        "\n",
        "              extract(item, arr)\n",
        "  \n",
        "        return arr\n",
        "\n",
        "    results = extract(obj, arr)\n",
        "\n",
        "    return modal, past, noun, verb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeY4l9PK6iNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scrap_from(obj, item):\n",
        "  word = str()\n",
        "  prev = str()\n",
        "  for f in obj:\n",
        "    if f['word'] == item['word']:\n",
        "      word = ' '\n",
        "    if len(word) > 0:\n",
        "      word = word + f['word']\n",
        "    if word != 's' and word != 'd' and word != '\\'re' and word != '\\'ve' and not (prev in '¥$€£' and word.isnumeric()) and word.lower() not in 'n\\'tmkx/' and word not in '..._^~=+*#:@;!?&/%':\n",
        "      prev = word\n",
        "      word = word + ' '  \n",
        "  return word    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm_UQ7ejvUsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def separate_children(obj):\n",
        "    arr = list()\n",
        "\n",
        "    def separate(obj, arr):\n",
        "        \n",
        "        if isinstance(obj, dict):\n",
        "\n",
        "            for k, v in obj.items():\n",
        "              if k != 'children':\n",
        "                if isinstance(v, (dict, list)):\n",
        "                    separate(v, arr)\n",
        "                elif k == 'word':      \n",
        "                    arr.append(v)\n",
        "\n",
        "        elif isinstance(obj, list):\n",
        "            for item in obj:\n",
        "                separate(item, arr)\n",
        "         \n",
        "        return arr\n",
        "\n",
        "    results = separate(obj, arr)\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XYFHp-5KyQ7",
        "colab": {}
      },
      "source": [
        "def extract_constituents():\n",
        " for k, v in prediction.items():\n",
        "  if (k=='hierplane_tree'):\n",
        "    for ke, val in v.items():\n",
        "      if (ke=='root'):\n",
        "        for key, value in val.items():\n",
        "          if (key == 'children'):\n",
        "            return extract_modal(value)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6BHdEZXkTu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def simplify(constit):\n",
        "  if isinstance(constit, str):\n",
        "    return constit\n",
        "  gist = list();\n",
        "  sentence = set()  # rebuild with checking for overlap\n",
        "  for c in constit:\n",
        "      #print(c)\n",
        "      p = separate_children(c) # candidate phrases\n",
        "      #print(p)\n",
        "      for head in list(p):\n",
        "        #print(head)\n",
        "        phrase = word_tokenize(head)\n",
        "        #print(phrase)\n",
        "        words = set(phrase)\n",
        "        if len(sentence.intersection(words)) < len(words):\n",
        "          sentence.update(words)\n",
        "          gist.extend(phrase)\n",
        "  result = str()\n",
        "  prev = str()\n",
        "  for word in gist:\n",
        "    if not (word.startswith('s ') or word.startswith('d ') or word.startswith('ve ') or word.startswith('re ')) and not (prev in '¥$€£' and word.isnumeric()) and word not in '..._^~=+*#:@;!?&/%':\n",
        "      prev = word\n",
        "      result = result + ' '\n",
        "    result = result + word  \n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aVJqHY-39ny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deduplicate(past):\n",
        "  for p1 in past:\n",
        "    for p2 in past:\n",
        "      if p2 in p1 and len(p1) > len(p2):\n",
        "        past.remove(p2)   \n",
        "\n",
        "  for p2 in past:\n",
        "    for p1 in past:\n",
        "      if p1 in p2 and len(p2) > len(p1):\n",
        "        past.remove(p1)\n",
        "         \n",
        "  return past         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_OCij-o85BT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_from(past, modal, empty=1):\n",
        "  antecedents = past.copy()\n",
        "\n",
        "  for m in sorted(modal,key=len): # in order of longer consequents\n",
        "    for p in reversed(sorted(past,key=len)): # shorter antecedents\n",
        "        if p in m and len(m) > len(p): \n",
        "          if p in past and len(past) > empty: # can't be left empty!\n",
        "            past.remove(p)  \n",
        "          \n",
        "  for a in antecedents:\n",
        "    for c in modal: # consequents (i.e. potential future)\n",
        "      if c in a: # a{c} wrapper around(potential) future\n",
        "        if a in past and len(past) > empty:\n",
        "            past.remove(a) # remove the pa{}st wrapper\n",
        "            #print(a) # was problematic in some cases \n",
        "\n",
        "  return past          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUKIAsKb9Cwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def defragment(past, text):\n",
        "  prev = str() # previous fragment\n",
        "  start = -1\n",
        "  end = 0\n",
        "  for p in reversed(sorted(past,key=len)):\n",
        "    r = p.lstrip()\n",
        "    #r = r.replace('--',' -- ')\n",
        "    r = r.replace(' - ','-')\n",
        "    r = r.replace('   ',' ')\n",
        "    r = r.replace('  ',' ')\n",
        "    r = r.replace(' .','.')\n",
        "    r = r.replace(' ,',',')\n",
        "    r = r.replace('( ','(')\n",
        "    r = r.replace(' )',')')\n",
        "    r = r.replace('$ ','$')\n",
        "    r = r.replace(' %','%')\n",
        "    r = r.replace('/ ','/')\n",
        "    r = r.replace(' /','/')\n",
        "    r = r.replace('Mr.','Mr. ')\n",
        "    r = r.replace('Ms.','Ms. ')\n",
        "    r = r.replace('Mrs.','Mrs. ')\n",
        "    r = r.replace('.  ','. ') \n",
        "    r = r.replace(' ;',';')\n",
        "    r = r.replace(' nt','nt')\n",
        "    try: \n",
        "      start = text.index(r)\n",
        "      #print(prev+' | '+r)\n",
        "      index = text.find(prev+r)\n",
        "      #print(index, end)\n",
        "      if index >= end and len(word_tokenize(p)) > 1:\n",
        "        prev = text[end:start]\n",
        "        end = start + len(r)\n",
        "      else: \n",
        "        past.remove(p)  \n",
        "    except ValueError:\n",
        "        print('{'+r+'} not in: '+text)\n",
        "  return past       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFKGDv6OwVUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def separate_from(md,pa):\n",
        "  if pa in md and len(md) > len(pa):\n",
        "    if md.index(pa) > 0 :\n",
        "      md = md[0:md.index(pa)]\n",
        "      #print('|'+md+'{...}')\n",
        "    else:\n",
        "      md = md.replace(pa,'')\n",
        "      #print('|{...}'+md)  \n",
        "  return md        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1os8EQvxh7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "original = \"If we were really well-known and went out and advertised, our team would spend more time reading about opportunities we don't like, he said.\"\n",
        "\n",
        "verbs = list()\n",
        "modal = list()\n",
        "nounm = list()\n",
        "past = list() \n",
        "nounp = list()\n",
        "nouns = list()\n",
        "\n",
        "#texts = original.rsplit(',') # chunck the sentence\n",
        "#for text in texts:\n",
        "\n",
        "text = original #.replace(',', ' ')\n",
        "text = text.replace('\\\"','') # remove double quotes\n",
        "text = text.replace('\\'','') # remove quotes\n",
        "#print(text)\n",
        "\n",
        "prediction = predictor.predict(sentence=text)\n",
        "mod, pas, nou, ver = extract_constituents()\n",
        "\n",
        "for v in ver:\n",
        "  vr = simplify(v) \n",
        "  if vr not in verbs:\n",
        "    verbs.append(vr)\n",
        "\n",
        "for u in nou:\n",
        "  nu = simplify(u) \n",
        "  if nu not in nouns:\n",
        "    nouns.append(nu) \n",
        "\n",
        "if len(mod)==0: # if no modal('MD') detected \n",
        "  for p in pas:\n",
        "    pa = simplify(p)\n",
        "    if pa not in past:\n",
        "        past.append(pa) \n",
        "\n",
        "for m in reversed(sorted(mod,key=len)):\n",
        "  md = simplify(m) # get one string gist\n",
        "  if len(md)==0:\n",
        "    continue\n",
        "  #print('=======================')  \n",
        "  #print(md)\n",
        "  \n",
        "  # separate NPs originating with MDs\n",
        "  nom = str()\n",
        "  for nm in reversed(sorted(nouns,key=len)):  \n",
        "    if nm in md:   \n",
        "      nounm.append(nm)\n",
        "      nouns.remove(nm)\n",
        "      nom = nm\n",
        "  \n",
        "  for p in pas:\n",
        "    pa = simplify(p)\n",
        "    if len(pa)==0:\n",
        "      continue\n",
        "    #print('-----------------------')\n",
        "    #print(pa)\n",
        "\n",
        "    new = separate_from(md, pa)\n",
        "    if new != md and len(new) > 0:\n",
        "       if md in modal: # remove original\n",
        "          modal.remove(md) \n",
        "       md = new \n",
        "\n",
        "    new = separate_from(pa, md)\n",
        "    if new != pa and len(new) > 0:\n",
        "       if pa in past: # remove original\n",
        "          past.remove(pa) \n",
        "       pa = new \n",
        "\n",
        "    if len(nom) > 0:\n",
        "      new = separate_from(pa, nom)\n",
        "      if new != pa and len(new) > 0:\n",
        "        if pa in past: # remove original\n",
        "            past.remove(pa) \n",
        "        pa = new    \n",
        "\n",
        "    # separate NPs originating with past tense      \n",
        "    for np in reversed(sorted(nouns,key=len)):\n",
        "      if np in pa:\n",
        "        nounp.append(np)\n",
        "        nouns.remove(np)\n",
        "       \n",
        "        #new = separate_from(pa, np)\n",
        "        #if new != pa and len(new) > 0:\n",
        "        #  if pa in past: # remove original\n",
        "        #      past.remove(pa) \n",
        "        #  pa = new \n",
        "      \n",
        "    if pa not in past and len(pa) > 0:\n",
        "      past.append(pa)         \n",
        "\n",
        "  if md not in modal and len(md) > 0:        \n",
        "    modal.append(md)\n",
        "\n",
        "verbs = deduplicate(verbs)\n",
        "nounm = deduplicate(nounm)\n",
        "nounp = deduplicate(nounp)\n",
        "modal = deduplicate(modal)\n",
        "past = deduplicate(past)\n",
        "\n",
        "modal = remove_from(modal, past)\n",
        "modal = remove_from(modal, nounm)\n",
        "past = remove_from(past, modal)\n",
        "\n",
        "nounm = remove_from(nounm, modal,0)\n",
        "nounp = remove_from(nounp, past, 0)\n",
        "nounm = remove_from(nounm, verbs,0)\n",
        "nounp = remove_from(nounp, verbs,0)\n",
        "\n",
        "past = remove_from(past, nounp)\n",
        "if len(past) > 1:\n",
        "  past = defragment(past, text) \n",
        "\n",
        "verbs = remove_from(verbs, modal)\n",
        "verbs = remove_from(verbs, past)\n",
        "\n",
        "print(nouns)\n",
        "print(verbs)\n",
        "print(nounm)\n",
        "print(nounp)\n",
        "print(past)\n",
        "print(modal)     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHIQLmP3AnyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from time import sleep\n",
        "\n",
        "prefix = '/content/'\n",
        "test_df = pd.read_csv(prefix + 'test.csv', header=None)\n",
        "test_df = test_df.drop(index=0)\n",
        "  \n",
        "test_df = pd.DataFrame({\n",
        "    'id':test_df[0],\n",
        "    'text': test_df[1], \n",
        "    'labels': ''\n",
        "})\n",
        "\n",
        "count = test_df['id'].count()\n",
        "print(count)\n",
        "\n",
        "i = 0\n",
        "j = 0\n",
        "modal_df= pd.DataFrame(columns=['id','labels','text'])\n",
        "past_df = pd.DataFrame(columns=['id','labels','text'])\n",
        "simple_df = pd.DataFrame(columns=['id','labels','text'])\n",
        "\n",
        "for ind in tqdm(test_df.index): \n",
        "  original = test_df['text'][ind]\n",
        "\n",
        "  verbs = list()\n",
        "  modal = list()\n",
        "  nounm = list()\n",
        "  past = list() \n",
        "  nounp = list()\n",
        "  nouns = list()\n",
        "\n",
        "  text = original #.replace(',', ' ')\n",
        "  text = text.replace('\\\"','') # remove double quotes\n",
        "  text = text.replace('\\'','') # remove quotes\n",
        "\n",
        "  prediction = predictor.predict(sentence=text)\n",
        "  mod, pas, nou, ver = extract_constituents()\n",
        "\n",
        "  for v in ver:\n",
        "    vr = simplify(v) \n",
        "    if vr not in verbs:\n",
        "      verbs.append(vr)\n",
        "\n",
        "  for u in nou:\n",
        "    nu = simplify(u) \n",
        "    if nu not in nouns:\n",
        "      nouns.append(nu) \n",
        "\n",
        "  if len(mod)==0: # if no modal('MD') detected \n",
        "    for p in pas:\n",
        "      pa = simplify(p)\n",
        "      if pa not in past:\n",
        "          past.append(pa) \n",
        "\n",
        "  for m in reversed(sorted(mod,key=len)):\n",
        "    md = simplify(m) # get one string gist\n",
        "    #print(md)\n",
        "\n",
        "    # separate NPs originating with MDs\n",
        "    nom = str()\n",
        "    for nm in reversed(sorted(nouns,key=len)):  \n",
        "      if nm in md:   \n",
        "        nounm.append(nm)\n",
        "        nouns.remove(nm)\n",
        "        nom = nm\n",
        "    \n",
        "    for p in pas:\n",
        "      pa = simplify(p)\n",
        "      if len(pa)==0:\n",
        "        continue\n",
        "      #print('-----------------------')\n",
        "      #print(pa)\n",
        "\n",
        "      new = separate_from(md, pa)\n",
        "      if new != md and len(new) > 0:\n",
        "        if md in modal: # remove original\n",
        "            modal.remove(md) \n",
        "        md = new \n",
        "\n",
        "      new = separate_from(pa, md)\n",
        "      if new != pa and len(new) > 0:\n",
        "        if pa in past: # remove original\n",
        "            past.remove(pa) \n",
        "        pa = new \n",
        "\n",
        "      if len(nom) > 0:\n",
        "        new = separate_from(pa, nom)\n",
        "        if new != pa and len(new) > 0:\n",
        "          if pa in past: # remove original\n",
        "              past.remove(pa) \n",
        "          pa = new    \n",
        "\n",
        "      # separate NPs originating with past tense      \n",
        "      for np in reversed(sorted(nouns,key=len)):\n",
        "        if np in pa:\n",
        "          nounp.append(np)\n",
        "          nouns.remove(np)\n",
        "        \n",
        "      if pa not in past and len(pa) > 0:\n",
        "        past.append(pa)         \n",
        "\n",
        "    if md not in modal and len(md) > 0:        \n",
        "      modal.append(md)\n",
        "\n",
        "  verbs = deduplicate(verbs)\n",
        "  nounm = deduplicate(nounm)\n",
        "  nounp = deduplicate(nounp)\n",
        "  modal = deduplicate(modal)\n",
        "  past = deduplicate(past)\n",
        "\n",
        "  modal = remove_from(modal, past)\n",
        "  modal = remove_from(modal, nounm)\n",
        "  past = remove_from(past, modal)\n",
        "\n",
        "  nounm = remove_from(nounm, modal,0)\n",
        "  nounp = remove_from(nounp, past, 0)\n",
        "  nounm = remove_from(nounm, verbs,0)\n",
        "  nounp = remove_from(nounp, verbs,0)\n",
        "\n",
        "  past = remove_from(past, nounp)\n",
        "  if len(past) > 1:\n",
        "    past = defragment(past, text) \n",
        "\n",
        "  verbs = remove_from(verbs, modal) # not used below but can be! \n",
        "  verbs = remove_from(verbs, past)\n",
        "\n",
        "  #assert len(modal) > 0 # can be legit (eg: 203585)\n",
        "  for m in modal:\n",
        "    m_row = pd.Series(data={\n",
        "      'id': test_df['id'][ind]+'C'+ str(i),\n",
        "      'labels':'0',\n",
        "      'text': m\n",
        "    }, name = str(i) )\n",
        "    modal_df = modal_df.append(m_row)\n",
        "\n",
        "    who = str()\n",
        "    for n in nounp:\n",
        "      who = who  +  n + '/'\n",
        "\n",
        "    what = str()\n",
        "    for nn in nounm:\n",
        "      what = what + '/' + nn\n",
        "\n",
        "    if len(past) > 0:\n",
        "      for p in past:\n",
        "        p_row = pd.Series(data={\n",
        "          'id': test_df['id'][ind]+'A'+ str(j),\n",
        "          'labels':'0',\n",
        "          'text': p\n",
        "        }, name = str(j) )\n",
        "        past_df = past_df.append(p_row)   \n",
        "        j = j+1   \n",
        "\n",
        "        s_row = pd.Series(data={\n",
        "          'id': test_df['id'][ind]+'A'+ str(j) + 'C'+ str(i),\n",
        "          'labels':'0',\n",
        "          'text': who + p + ' | ' + m + what\n",
        "        }, name = str(i) )\n",
        "        simple_df = simple_df.append(s_row) \n",
        "    else:\n",
        "      s_row = pd.Series(data={\n",
        "        'id': test_df['id'][ind]+'A'+ str(j) + 'C'+ str(i),\n",
        "        'labels':'0',\n",
        "        'text': who + ' | ' + m + what\n",
        "      }, name = str(i) )\n",
        "      simple_df = simple_df.append(s_row)\n",
        "\n",
        "    i = i+1\n",
        "\n",
        "  sleep(1)  \n",
        "\n",
        "past_df.to_csv(prefix+'antecedents.tsv', sep='\\t', index=False, header=False)\n",
        "modal_df.to_csv(prefix+'consequents.tsv', sep='\\t', index=False, header=False)\n",
        "simple_df.to_csv(prefix+'simplified.tsv', sep='\\t', index=False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3w26wdnuriO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv /content/antecedents.tsv /content/gdrive/My\\ Drive/Colab\\ Notebooks/Subtask-2/\n",
        "!mv /content/consequents.tsv /content/gdrive/My\\ Drive/Colab\\ Notebooks/Subtask-2/\n",
        "!mv /content/simplified.tsv /content/gdrive/My\\ Drive/Colab\\ Notebooks/Subtask-2/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}